{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --quiet --user --force-reinstall datasets==2.18.0 pandas==2.2.1 numpy==1.26.4 nltk==3.8.1 levenshtein==0.25.0 sentence-transformers==2.5.1 transformers==4.38.2 openai==1.14.1 gdown==5.1.0 torch==2.2.1 umap-learn==0.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir benchmark\n",
    "%mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown \"1Cfr6GZZps-HZtTqPRMAXPqzKqsGjUsNR\" --output \"data/test.contr.json\" --quiet\n",
    "!gdown \"1xP35a1Y_S0-LBT_bg8aubSDShIIOp4Zg\" --output \"data/test.uncontr.json\" --quiet\n",
    "!gdown \"1ouoV1-MBc5zxycrNcYKUwLfcF5EjYG__\" --output \"data/train.contr.json\" --quiet\n",
    "!gdown \"14vcPWyyCt9zCWXJaONdizB7eJovE4EUY\" --output \"data/train.uncontr.json\" --quiet\n",
    "!gdown \"1HSpe4ERtfpATjkff3E2lvXFneA-VUk5q\" --output \"data/validation.contr.json\" --quiet\n",
    "!gdown \"18CnNKT2JZOoVYkxbDzShCYxB0u338UyM\" --output \"data/validation.uncontr.json\" --quiet\n",
    "\n",
    "!gdown \"153AaDrTO3x354AjvLNeHtz-fojBioJ7m\" --output \"data/One2Set.txt\" --quiet\n",
    "!gdown \"13HkAz5WItwVKbvdJekI2stkUdKoDfrTt\" --output \"data/CatSeqTG_2RF1.txt\" --quiet\n",
    "!gdown \"1mDM1JGlsIndzbGpY6x5_5euLYWMXaG2J\" --output \"data/KG-KE-KR-M.out\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def contains(subseq, inseq):\n",
    "    return any(inseq[pos:pos + len(subseq)] == subseq for pos in range(0, len(inseq) - len(subseq) + 1))\n",
    "\n",
    "def tokenize(s):\n",
    "    return word_tokenize(s)\n",
    "\n",
    "def lowercase_and_stem(_words):\n",
    "    return [stemmer.stem(w.lower()) for w in _words]\n",
    "\n",
    "def pmru(tok_text, tok_kps):\n",
    "    p, r, m, u = [], [], [], []\n",
    "    absent_words = set()\n",
    "    # loop through the keyphrases\n",
    "    for j, kp in enumerate(tok_kps):\n",
    "\n",
    "        # if kp is present\n",
    "        if contains(kp, tok_text):\n",
    "            p.append(j)\n",
    "\n",
    "        # if kp is considered as absent\n",
    "        else:\n",
    "            # find present and absent words\n",
    "            present_words = [w for w in kp if w in tok_text]\n",
    "            absent_words.update([w for w in kp if w not in tok_text])\n",
    "\n",
    "            # if \"all\" words are present\n",
    "            if len(present_words) == len(kp):\n",
    "                r.append(j)\n",
    "            # if \"some\" words are present\n",
    "            elif len(present_words) > 0:\n",
    "                m.append(j)\n",
    "            # if \"no\" words are present\n",
    "            else:\n",
    "                u.append(j)\n",
    "\n",
    "    return {\"P\": p, \"R\": r, \"M\": m, \"U\": u}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas\n",
    "import numpy\n",
    "import json\n",
    "\n",
    "controlled = [\"controlled\", \"uncontrolled\"]\n",
    "prmu = [\"P\", \"R\", \"M\", \"U\"]\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "dataset = pandas.DataFrame()\n",
    "\n",
    "for split in splits:\n",
    "    keyphrases = { \"controlled\": json.load(open(f\"data/{split}.contr.json\")), \"uncontrolled\": json.load(open(f\"data/{split}.uncontr.json\")) }\n",
    "    split_df = load_dataset(\"taln-ls2n/inspec\", split=split, trust_remote_code=True).to_pandas()\n",
    "    split_df[\"split\"] = split\n",
    "    split_df[\"input\"] = split_df[[\"title\", \"abstract\"]].apply(lambda x: (x[\"title\"] + \". \" + x[\"abstract\"]).lower() if x[\"title\"][-1].isalpha() else (x[\"title\"] + \" \" + x[\"abstract\"]).lower(), axis=1)\n",
    "    for contr in controlled:\n",
    "        split_df[f\"keyphrases_{contr}\"] = split_df[[\"id\"]].apply(lambda x: [\" \".join(keyphrase) for keyphrase in keyphrases[contr][x[\"id\"]]], axis=1)\n",
    "        split_df[f\"prmu_{contr}\"] = split_df[[\"input\", f\"keyphrases_{contr}\"]].apply(lambda x: pmru(tok_text=lowercase_and_stem(tokenize(x[\"input\"])), tok_kps=[lowercase_and_stem(tokenize(keyphrase)) for keyphrase in x[f\"keyphrases_{contr}\"]]), axis=1)\n",
    "        split_df[contr] = split_df[[f\"keyphrases_{contr}\", f\"prmu_{contr}\"]].apply(lambda x: { key: numpy.array(x[f\"keyphrases_{contr}\"])[x[f\"prmu_{contr}\"][key]] for key in prmu }, axis=1)\n",
    "    dataset = pandas.concat([dataset, split_df]).sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "dataset = dataset[[\"id\", \"split\", \"input\"] + controlled]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics about each output vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "\n",
    "model_name=\"all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "metrics = {\n",
    "    \"cosine\": cosine_similarity, \"euclidean\": euclidean_distances, \"manhattan\": manhattan_distances\n",
    "}\n",
    "\n",
    "def pairwise_distances(index):\n",
    "    embeddings = model.encode(index, convert_to_tensor=True)\n",
    "    return {\n",
    "        distance: metrics[distance](embeddings.numpy(), embeddings.numpy()).mean().round(2) for distance in [\"euclidean\", \"manhattan\", \"cosine\"]\n",
    "    }\n",
    "    \n",
    "stemmed = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pandas.DataFrame()\n",
    "\n",
    "for contr in controlled:\n",
    "    for stem in stemmed:\n",
    "        vocabulary = [\" \".join([stemmer.stem(word) for word in tokenize(keyphrase)]) if stem else keyphrase for keyphrase in \",\".join(dataset[contr].apply(lambda x: \",\".join([\",\".join(x[key]) for key in prmu if len(x[key]) > 0])).values).split(\",\")]\n",
    "        ngrams = [len(tokenize(keyphrase)) for keyphrase in vocabulary]\n",
    "        table = pandas.DataFrame({\n",
    "            \"thesaurus\": contr + \" (pre-stemmed)\" if stem else contr,\n",
    "            \"n_unique_keyphrases\": numpy.unique(vocabulary).size.__str__(),\n",
    "            \"n_occurence_keyphrases\": f\"\"\"{numpy.mean(list(Counter(vocabulary).values())).round(2)} ± {numpy.std(list(Counter(vocabulary).values())).round(2)}\"\"\",\n",
    "            \"n_grams_keyphrases\": f\"\"\"{numpy.mean(ngrams).round(2)} ± {numpy.std(ngrams).round(2)}\"\"\",\n",
    "        }, index=[0])\n",
    "        \n",
    "        tables = pandas.concat([tables, table]).reset_index(drop=True)\n",
    "\n",
    "print(tables.to_latex())\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.concat([pandas.DataFrame(Counter([len(tokenize(keyphrase)) for keyphrase in numpy.unique([k for k in \",\".join(dataset[\"controlled\"].apply(lambda x: \",\".join([\",\".join(x[key]) for key in prmu if len(x[key]) > 0])).values).split(\",\")])]).items()).set_index(0).sort_index().transpose(),\n",
    "               pandas.DataFrame(Counter([len(tokenize(keyphrase)) for keyphrase in numpy.unique([k for k in \",\".join(dataset[\"uncontrolled\"].apply(lambda x: \",\".join([\",\".join(x[key]) for key in prmu if len(x[key]) > 0])).values).split(\",\")])]).items()).set_index(0).sort_index().transpose()]).set_axis([\"controlled\", \"uncontrolled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.concat([pandas.DataFrame(Counter([len(tokenize(keyphrase)) for keyphrase in numpy.unique([k for k in \",\".join(dataset[\"controlled\"].apply(lambda x: \",\".join([\",\".join(x[key]) for key in prmu if len(x[key]) > 0])).values).split(\",\")])]).items()).set_index(0).sort_index().transpose() / 2059,\n",
    "               pandas.DataFrame(Counter([len(tokenize(keyphrase)) for keyphrase in numpy.unique([k for k in \",\".join(dataset[\"uncontrolled\"].apply(lambda x: \",\".join([\",\".join(x[key]) for key in prmu if len(x[key]) > 0])).values).split(\",\")])]).items()).set_index(0).sort_index().transpose() / 16916]).set_axis([\"controlled\", \"uncontrolled\"]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([k for k in \",\".join(dataset[\"controlled\"].apply(lambda x: \",\".join([\",\".join(x[key]) for key in prmu if len(x[key]) > 0])).values).split(\",\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([k for k in \",\".join(dataset[\"uncontrolled\"].apply(lambda x: \",\".join([\",\".join(x[key]) for key in prmu if len(x[key]) > 0])).values).split(\",\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pandas.DataFrame()\n",
    "\n",
    "for contr in controlled:\n",
    "    n_keyphrases = dataset[contr].apply(lambda x: sum([len(x[key]) for key in prmu]))\n",
    "    distances = dataset[contr].apply(lambda x: pairwise_distances([\",\".join(x[key]) for key in prmu]))\n",
    "    distribution = {\n",
    "        key: f\"{dataset[contr].apply(lambda x: 100 * len(x[key]) / sum([len(x[k]) for k in prmu])).mean().round(2)}%\" for key in prmu\n",
    "    }\n",
    "    table = pandas.DataFrame({\n",
    "        \"thesaurus\": contr,\n",
    "        \"n_keyphrases\": f\"\"\"{numpy.mean(n_keyphrases).round(2)} ± {numpy.std(n_keyphrases).round(2)}\"\"\"\n",
    "    } | {\n",
    "        distance: f\"\"\"{int(100 * numpy.mean(distances.apply(lambda x: x[distance])).round(2)) / 100} ± {int(100 * numpy.std(distances.apply(lambda x: x[distance])).round(2)) / 100}\"\"\" for distance in [\"euclidean\", \"manhattan\", \"cosine\"]\n",
    "    } | distribution, index=[0])\n",
    "    \n",
    "    tables = pandas.concat([tables, table]).reset_index(drop=True)\n",
    "\n",
    "print(tables.to_latex())\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pandas.DataFrame()\n",
    "\n",
    "for split in splits:\n",
    "    for contr in [\"controlled\"]:\n",
    "        vocabulary = [\" \".join([stemmer.stem(word) for word in tokenize(keyphrase)]) if stem else keyphrase for keyphrase in \",\".join(dataset[dataset[\"split\"] == split][contr].apply(lambda x: \",\".join([\",\".join(x[key]) for key in prmu if len(x[key]) > 0])).values).split(\",\")]\n",
    "        other_vocabulary = [\" \".join([stemmer.stem(word) for word in tokenize(keyphrase)]) if stem else keyphrase for keyphrase in \",\".join(dataset[dataset[\"split\"] != split][contr].apply(lambda x: \",\".join([\",\".join(x[key]) for key in prmu if len(x[key]) > 0])).values).split(\",\")]\n",
    "        n_keyphrases = dataset[dataset[\"split\"] == split][contr].apply(lambda x: sum([len(x[key]) for key in prmu]))\n",
    "        sentences = dataset[dataset[\"split\"] == split][\"input\"].apply(lambda x: len(sent_tokenize(x)))\n",
    "        size = len(dataset[dataset[\"split\"] == split])\n",
    "        distribution = {\n",
    "            key: f\"\"\"{dataset[dataset[\"split\"] == split][contr].apply(lambda x: 100 * len(x[key]) / sum([len(x[k]) for k in prmu])).mean().round(2)}%\"\"\" for key in prmu\n",
    "        }\n",
    "        table = pandas.DataFrame({\n",
    "            \"split\": split,\n",
    "            \"n_documents\": size,\n",
    "            \"n_sentences\": f\"\"\"{numpy.mean(sentences).round(2)} ± {numpy.std(sentences).round(2)}\"\"\",\n",
    "            \"n_keyphrases\": f\"\"\"{numpy.mean(n_keyphrases).round(2)} ± {numpy.std(n_keyphrases).round(2)}\"\"\",\n",
    "            \"coverage_keyphrases\": f\"\"\"{round(100 * len(numpy.unique(vocabulary)) / 2059, 2)}%\"\"\",\n",
    "            \"exclusive_keyphrases\": f\"\"\"{round(100 * len(set(vocabulary) - set(other_vocabulary)) / 2059, 2)}%\"\"\"\n",
    "        } | distribution, index=[0])\n",
    "        \n",
    "        tables = pandas.concat([tables, table]).reset_index(drop=True)\n",
    "\n",
    "print(tables.to_latex())\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from datasets import load_dataset\n",
    "from itertools import chain\n",
    "import pandas\n",
    "import numpy\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "controlled = [\"controlled\"]\n",
    "prmu = [\"P\", \"R\", \"M\", \"U\"]\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "dataset = pandas.DataFrame()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def contains(subseq, inseq):\n",
    "    return any(inseq[pos:pos + len(subseq)] == subseq for pos in range(0, len(inseq) - len(subseq) + 1))\n",
    "\n",
    "def tokenize(s):\n",
    "    return word_tokenize(s)\n",
    "\n",
    "def lowercase_and_stem(_words):\n",
    "    return [stemmer.stem(w.lower()) for w in _words]\n",
    "\n",
    "def pmru(tok_text, tok_kps):\n",
    "    p, r, m, u = [], [], [], []\n",
    "    absent_words = set()\n",
    "    # loop through the keyphrases\n",
    "    for j, kp in enumerate(tok_kps):\n",
    "\n",
    "        # if kp is present\n",
    "        if contains(kp, tok_text):\n",
    "            p.append(j)\n",
    "\n",
    "        # if kp is considered as absent\n",
    "        else:\n",
    "            # find present and absent words\n",
    "            present_words = [w for w in kp if w in tok_text]\n",
    "            absent_words.update([w for w in kp if w not in tok_text])\n",
    "\n",
    "            # if \"all\" words are present\n",
    "            if len(present_words) == len(kp):\n",
    "                r.append(j)\n",
    "            # if \"some\" words are present\n",
    "            elif len(present_words) > 0:\n",
    "                m.append(j)\n",
    "            # if \"no\" words are present\n",
    "            else:\n",
    "                u.append(j)\n",
    "\n",
    "    return {\"P\": p, \"R\": r, \"M\": m, \"U\": u}\n",
    "\n",
    "for split in splits:\n",
    "    keyphrases = { \"controlled\": json.load(open(f\"data/{split}.contr.json\")) }\n",
    "    split_df = load_dataset(\"taln-ls2n/inspec\", split=split, trust_remote_code=True).to_pandas()\n",
    "    split_df[\"split\"] = split\n",
    "    split_df[\"input\"] = split_df[[\"title\", \"abstract\"]].apply(lambda x: (x[\"title\"] + \". \" + x[\"abstract\"]).lower() if x[\"title\"][-1].isalpha() else (x[\"title\"] + \" \" + x[\"abstract\"]).lower(), axis=1)\n",
    "    for contr in controlled:\n",
    "        split_df[f\"keyphrases_{contr}\"] = split_df[[\"id\"]].apply(lambda x: [\" \".join(keyphrase) for keyphrase in keyphrases[contr][x[\"id\"]]], axis=1)\n",
    "        split_df[f\"prmu_{contr}\"] = split_df[[\"input\", f\"keyphrases_{contr}\"]].apply(lambda x: pmru(tok_text=lowercase_and_stem(tokenize(x[\"input\"])), tok_kps=[lowercase_and_stem(tokenize(keyphrase)) for keyphrase in x[f\"keyphrases_{contr}\"]]), axis=1)\n",
    "        split_df[contr] = split_df[[f\"keyphrases_{contr}\", f\"prmu_{contr}\"]].apply(lambda x: { key: numpy.array(x[f\"keyphrases_{contr}\"])[x[f\"prmu_{contr}\"][key]] for key in prmu }, axis=1)\n",
    "    dataset = pandas.concat([dataset, split_df]).sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "indexes = numpy.unique(\",\".join(dataset[contr].apply(lambda x: \",\".join([\",\".join(x[key]) for key in prmu if len(x[key]) > 0])).values).split(\",\"))\n",
    "dataset = dataset[[\"id\", \"split\", \"input\", contr]]\n",
    "dataset[contr] = dataset[contr].apply(lambda x: {\n",
    "    key: set([\" \".join([stemmer.stem(word) for word in tokenize(keyphrase)]) for keyphrase in x[key]]) if len(x[key]) > 0 else set() for key in prmu\n",
    "})\n",
    "dataset = dataset[dataset[\"split\"] == \"test\"]\n",
    "indices = dataset.index\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(keyphrases, references):\n",
    "    O = sum([len(references[key]) for key in prmu])\n",
    "    P = {\n",
    "        key: {\n",
    "            \"5\": 0 if len(keyphrases[:5]) == 0 else len(set(keyphrases[:5]) & references[key]) / len(keyphrases[:5]) if len(references[key]) != 0 else numpy.nan,\n",
    "            \"O\": 0 if len(keyphrases[:5]) == 0 else len(set(keyphrases[:O]) & references[key]) / len(keyphrases[:O]) if len(references[key]) != 0 else numpy.nan\n",
    "        } for key in prmu\n",
    "    }\n",
    "    R = {\n",
    "        key: {\n",
    "            \"5\": 0 if len(keyphrases[:5]) == 0 else len(set(keyphrases[:5]) & references[key]) / len(references[key]) if len(references[key]) != 0 else numpy.nan,\n",
    "            \"O\": 0 if len(keyphrases[:5]) == 0 else len(set(keyphrases[:O]) & references[key]) / len(references[key]) if len(references[key]) != 0 else numpy.nan\n",
    "        } for key in prmu\n",
    "    }\n",
    "    F = {\n",
    "        key: {\n",
    "            nkeys: 0 if len(keyphrases[:5]) == 0 else numpy.nan if len(references[key]) == 0 else (2*P[key][nkeys]*R[key][nkeys])/(P[key][nkeys]+R[key][nkeys]) if (P[key][nkeys]+R[key][nkeys]) > 0 else 0 for nkeys in P[key].keys()\n",
    "        } for key in prmu\n",
    "    }\n",
    "    return {\n",
    "        f\"{key}_P_{nkeys}\": P[key][nkeys] for key in prmu for nkeys in P[key].keys()\n",
    "    } | {\n",
    "        f\"{key}_R_{nkeys}\": R[key][nkeys] for key in prmu for nkeys in R[key].keys()\n",
    "    } | {\n",
    "        f\"{key}_F_{nkeys}\": F[key][nkeys] for key in prmu for nkeys in F[key].keys()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pke.unsupervised import MultipartiteRank, PositionRank, YAKE, TopicRank, TfIdf, KPMiner\n",
    "\n",
    "for model in [MultipartiteRank, PositionRank, YAKE, TopicRank, TfIdf, KPMiner]:\n",
    "    results = pandas.DataFrame({\n",
    "        \"model\": [],\n",
    "        \"embedded\": [],\n",
    "        \"id\": [],\n",
    "        \"keyphrases\": []\n",
    "    } | {\n",
    "        f\"{key}_{metric}_{nkeys}\" : [] for metric in [\"P\", \"R\", \"F\"] for key in prmu for nkeys in [\"5\", \"O\"]\n",
    "    } | {\n",
    "        f\"{key}_correct_O\" : [] for key in prmu\n",
    "    })\n",
    "    extractor = model()\n",
    "\n",
    "    for id in indices:\n",
    "        references = dataset.loc[id, \"controlled\"]\n",
    "        O = sum([len(references[key]) for key in prmu])\n",
    "        extractor.load_document(input=dataset.loc[id, \"input\"], language=\"en\")\n",
    "        extractor.candidate_selection()\n",
    "        extractor.candidate_weighting()\n",
    "        predictions = extractor.get_n_best(n=max(5, sum([len(references[key]) for key in prmu])))\n",
    "        keyphrases = [\" \".join(lowercase_and_stem(tokenize(keyphrase[0]))) for keyphrase in predictions]\n",
    "\n",
    "        results = pandas.concat([results, pandas.DataFrame({\n",
    "            \"model\": [model.__name__],\n",
    "            \"embedded\": [\"False\"],\n",
    "            \"id\": [dataset.loc[id, \"id\"]],\n",
    "            \"keyphrases\": [\",\".join([keyphrase[0] for keyphrase in predictions])]\n",
    "        } | evaluate(keyphrases, references) | {\n",
    "            f\"{key}_correct_O\" : [len(set(keyphrases[:O]) & set(references[key]))] for key in prmu\n",
    "        })])\n",
    "    \n",
    "        results.to_csv(f\"benchmark/results_{model.__name__}.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer One2Seq-Paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "for model_name in [\"beogradjanka/bart_multitask_finetuned_for_title_and_keyphrase_generation\", \"bloomberg/KeyBART\"]:\n",
    "    results = pandas.DataFrame({\n",
    "        \"model\": [],\n",
    "        \"embedded\": [],\n",
    "        \"id\": [],\n",
    "        \"keyphrases\": []\n",
    "    } | {\n",
    "        f\"{key}_{metric}_{nkeys}\" : [] for metric in [\"P\", \"R\", \"F\"] for key in prmu for nkeys in [\"5\", \"O\"]\n",
    "    } | {\n",
    "        f\"{key}_correct_O\" : [] for key in prmu\n",
    "    })\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    if \"finetuned\" in model_name:\n",
    "        sep = \"\\n\"\n",
    "    else:\n",
    "        sep = \";\"\n",
    "\n",
    "    for id in indices:\n",
    "        references = dataset.loc[id, \"controlled\"]\n",
    "        O = sum([len(references[key]) for key in prmu])\n",
    "        tokenized_text = tokenizer.prepare_seq2seq_batch([\"<|KEYPHRASES|> \" + dataset.loc[id, \"input\"]], return_tensors='pt')\n",
    "        translation = model.generate(**tokenized_text)\n",
    "        translated_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]\n",
    "        predictions = translated_text.split(sep)\n",
    "        keyphrases = [\" \".join(lowercase_and_stem(tokenize(keyphrase))) for keyphrase in predictions]\n",
    "\n",
    "        results = pandas.concat([results, pandas.DataFrame({\n",
    "            \"model\": [model_name.split(\"/\")[-1].split(\"_\")[0]],\n",
    "            \"embedded\": [\"False\"],\n",
    "            \"id\": [dataset.loc[id, \"id\"]],\n",
    "            \"keyphrases\": [\",\".join([keyphrase for keyphrase in predictions])]\n",
    "        } | evaluate(keyphrases, references) | {\n",
    "            f\"{key}_correct_O\" : [len(set(keyphrases[:O]) & set(references[key]))] for key in prmu\n",
    "        })])\n",
    "    \n",
    "        results.to_csv(f\"\"\"benchmark/results_{model_name.split(\"/\")[-1].split(\"_\")[0]}.csv\"\"\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "model_name = \"ChatGPT\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint=#\n",
    "  api_key=#\n",
    "  api_version=#\n",
    ")\n",
    "\n",
    "results = pandas.DataFrame({\n",
    "    \"model\": [],\n",
    "    \"embedded\": [],\n",
    "    \"id\": [],\n",
    "    \"keyphrases\": []\n",
    "} | {\n",
    "    f\"{key}_{metric}_{nkeys}\" : [] for metric in [\"P\", \"R\", \"F\"] for key in prmu for nkeys in [\"5\", \"O\"]\n",
    "} | {\n",
    "    f\"{key}_correct_O\" : [] for key in prmu\n",
    "})\n",
    "sep=\",\"\n",
    "\n",
    "for id in indices:\n",
    "    references = dataset.loc[id, \"controlled\"]\n",
    "    O = sum([len(references[key]) for key in prmu])\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"oa-coeml-gpt-35-us\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You will be provided with a block of text, and your task is to extract a list of keywords from it. Separate each keyword with a comma.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": dataset.loc[id, \"input\"]\n",
    "        }\n",
    "      ],\n",
    "      temperature=0,\n",
    "      max_tokens=512,\n",
    "      top_p=1\n",
    "    )\n",
    "\n",
    "    predictions = response.choices[0].message.content.split(sep)\n",
    "    keyphrases = [\" \".join(lowercase_and_stem(tokenize(keyphrase))) for keyphrase in predictions]\n",
    "\n",
    "    results = pandas.concat([results, pandas.DataFrame({\n",
    "        \"model\": [model_name],\n",
    "        \"embedded\": [\"False\"],\n",
    "        \"id\": [dataset.loc[id, \"id\"]],\n",
    "        \"keyphrases\": [\",\".join([keyphrase for keyphrase in predictions])]\n",
    "    } | evaluate(keyphrases, references) | {\n",
    "        f\"{key}_correct_O\" : [len(set(keyphrases[:O]) & set(references[key]))] for key in prmu\n",
    "    })])\n",
    "\n",
    "    results.to_csv(f\"benchmark/results_{model_name}.csv\", sep=\";\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One2Set and CatSeqTG_2RF1 and KG-KE-KR-M\n",
    "\n",
    "We are using results from these repositories:\n",
    "\n",
    "[repo](https://github.com/jiacheng-ye/kg_one2set) related to this [paper](https://arxiv.org/abs/1906.04106)\n",
    "    \n",
    "[repo](https://github.com/kenchan0226/keyphrase-generation-rl?tab=readme-ov-file) related to this [paper](https://arxiv.org/abs/2105.11134) \n",
    "\n",
    "[repo](https://github.com/Chen-Wang-CUHK/KG-KE-KR-M) related to this [paper](https://arxiv.org/pdf/1904.03454.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"One2Set.txt\", \"CatSeqTG_2RF1.txt\", \"KG-KE-KR-M.out\"]:\n",
    "    all_predictions = open(f\"data/{model}\").readlines()\n",
    "\n",
    "    results = pandas.DataFrame({\n",
    "        \"model\": [],\n",
    "        \"embedded\": [],\n",
    "        \"id\": [],\n",
    "        \"keyphrases\": []\n",
    "    } | {\n",
    "        f\"{key}_{metric}_{nkeys}\" : [] for metric in [\"P\", \"R\", \"F\"] for key in prmu for nkeys in [\"5\", \"O\"]\n",
    "    } | {\n",
    "        f\"{key}_correct_O\" : [] for key in prmu\n",
    "    })\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        references = dataset.loc[indices[i], \"controlled\"]\n",
    "        O = sum([len(references[key]) for key in prmu])\n",
    "        if model == \"KG-KE-KR-M.out\":\n",
    "            predictions = all_predictions[i].split(\" ; \")\n",
    "        else:\n",
    "            predictions = all_predictions[i].split(\";\")\n",
    "        keyphrases = [\" \".join(lowercase_and_stem(tokenize(keyphrase))) for keyphrase in predictions]\n",
    "\n",
    "        results = pandas.concat([results, pandas.DataFrame({\n",
    "            \"model\": [model.split(\".\")[0]],\n",
    "            \"embedded\": [\"False\"],\n",
    "            \"id\": [dataset.loc[id, \"id\"]],\n",
    "            \"keyphrases\": [\",\".join([keyphrase for keyphrase in predictions])]\n",
    "        } | evaluate(keyphrases, references) | {\n",
    "            f\"{key}_correct_O\" : [len(set(keyphrases[:O]) & set(references[key]))] for key in prmu\n",
    "        })])\n",
    "\n",
    "        results.to_csv(f\"\"\"benchmark/results_{model.split(\".\")[0]}.csv\"\"\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding-based Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import umap\n",
    "\n",
    "corpus = indexes\n",
    "\n",
    "for model in [\"all-MiniLM-L12-v2\", \"multi-qa-mpnet-base-dot-v1\", \"all-mpnet-base-v2\"]:\n",
    "    embedder = SentenceTransformer(model)\n",
    "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True).numpy()\n",
    "    results = pandas.DataFrame({\n",
    "        \"model\": [],\n",
    "        \"embedded\": [],\n",
    "        \"id\": [],\n",
    "        \"keyphrases\": []\n",
    "    } | {\n",
    "        f\"{key}_{metric}_{nkeys}\" : [] for metric in [\"P\", \"R\", \"F\"] for key in prmu for nkeys in [\"5\", \"O\"]\n",
    "    } | {\n",
    "        f\"{key}_correct_O\" : [] for key in prmu\n",
    "    })\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        queries = sent_tokenize(dataset.loc[indices[i], \"input\"])\n",
    "        references = dataset.loc[indices[i], \"controlled\"]\n",
    "        O = sum([len(references[key]) for key in prmu])\n",
    "        k = 5\n",
    "\n",
    "        query_embeddings = embedder.encode(queries, convert_to_tensor=True).numpy()\n",
    "        for score in [\"cosine\"]:\n",
    "            predictions = []\n",
    "            if score == \"cosine\":\n",
    "                scores = torch.from_numpy(cosine_similarity(query_embeddings, corpus_embeddings))\n",
    "            elif score == \"manhattan\":\n",
    "                scores = torch.from_numpy(manhattan_distances(query_embeddings, corpus_embeddings))\n",
    "            else:\n",
    "                scores = torch.from_numpy(euclidean_distances(query_embeddings, corpus_embeddings))\n",
    "            top_results = torch.topk(scores, k=k, largest=(score==\"cosine\")).indices.numpy()\n",
    "            predictions.append(list(corpus[top_results]))\n",
    "\n",
    "            for top_k in [5]:\n",
    "                keyphrases = list(Counter([\" \".join(lowercase_and_stem(tokenize(keyphrase))) for keyphrase in list(chain(*[list(prediction[:top_k]) for prediction in predictions[0]]))]).keys())\n",
    "\n",
    "                results = pandas.concat([results, pandas.DataFrame({\n",
    "                    \"model\": [model],\n",
    "                    \"embedded\": [\"True\"],\n",
    "                    \"id\": [dataset.loc[id, \"id\"]],\n",
    "                    \"keyphrases\": [\",\".join(list(chain(*[list(prediction[:top_k]) for prediction in predictions[0]])))]\n",
    "                } | evaluate(keyphrases, references) | {\n",
    "                    f\"{key}_correct_O\" : [len(set(keyphrases[:O]) & set(references[key]))] for key in prmu\n",
    "                })])\n",
    "\n",
    "                results.to_csv(f\"benchmark/results_{model}.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>P_P_5</th>\n",
       "      <th>P_P_O</th>\n",
       "      <th>R_P_5</th>\n",
       "      <th>R_P_O</th>\n",
       "      <th>M_P_5</th>\n",
       "      <th>M_P_O</th>\n",
       "      <th>U_P_5</th>\n",
       "      <th>U_P_O</th>\n",
       "      <th>P_R_5</th>\n",
       "      <th>P_R_O</th>\n",
       "      <th>...</th>\n",
       "      <th>R_F_5</th>\n",
       "      <th>R_F_O</th>\n",
       "      <th>M_F_5</th>\n",
       "      <th>M_F_O</th>\n",
       "      <th>U_F_5</th>\n",
       "      <th>U_F_O</th>\n",
       "      <th>P_correct_O</th>\n",
       "      <th>R_correct_O</th>\n",
       "      <th>M_correct_O</th>\n",
       "      <th>U_correct_O</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>embedded</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CatSeqTG_2RF1</th>\n",
       "      <th>False</th>\n",
       "      <td>12.20</td>\n",
       "      <td>13.78</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.66</td>\n",
       "      <td>38.88</td>\n",
       "      <td>36.72</td>\n",
       "      <td>...</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.86</td>\n",
       "      <td>36.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGPT</th>\n",
       "      <th>False</th>\n",
       "      <td>7.73</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>24.12</td>\n",
       "      <td>21.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>23.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KG-KE-KR-M</th>\n",
       "      <th>False</th>\n",
       "      <td>15.85</td>\n",
       "      <td>16.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.37</td>\n",
       "      <td>47.80</td>\n",
       "      <td>...</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.25</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KPMiner</th>\n",
       "      <th>False</th>\n",
       "      <td>7.24</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>17.78</td>\n",
       "      <td>17.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>19.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KeyBART</th>\n",
       "      <th>False</th>\n",
       "      <td>8.13</td>\n",
       "      <td>9.60</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>23.23</td>\n",
       "      <td>22.43</td>\n",
       "      <td>...</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>22.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultipartiteRank</th>\n",
       "      <th>False</th>\n",
       "      <td>6.60</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>20.53</td>\n",
       "      <td>19.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One2Set</th>\n",
       "      <th>False</th>\n",
       "      <td>14.00</td>\n",
       "      <td>15.81</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>43.05</td>\n",
       "      <td>42.06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.72</td>\n",
       "      <td>42.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PositionRank</th>\n",
       "      <th>False</th>\n",
       "      <td>6.45</td>\n",
       "      <td>7.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.26</td>\n",
       "      <td>19.48</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TfIdf</th>\n",
       "      <th>False</th>\n",
       "      <td>7.92</td>\n",
       "      <td>8.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.18</td>\n",
       "      <td>25.22</td>\n",
       "      <td>22.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.26</td>\n",
       "      <td>24.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TopicRank</th>\n",
       "      <th>False</th>\n",
       "      <td>6.21</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>19.04</td>\n",
       "      <td>17.66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YAKE</th>\n",
       "      <th>False</th>\n",
       "      <td>6.71</td>\n",
       "      <td>7.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.19</td>\n",
       "      <td>21.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all-MiniLM-L12-v2</th>\n",
       "      <th>True</th>\n",
       "      <td>13.99</td>\n",
       "      <td>16.45</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.70</td>\n",
       "      <td>10.64</td>\n",
       "      <td>12.79</td>\n",
       "      <td>4.91</td>\n",
       "      <td>5.67</td>\n",
       "      <td>43.92</td>\n",
       "      <td>44.48</td>\n",
       "      <td>...</td>\n",
       "      <td>11.80</td>\n",
       "      <td>12.95</td>\n",
       "      <td>14.45</td>\n",
       "      <td>16.61</td>\n",
       "      <td>6.70</td>\n",
       "      <td>7.30</td>\n",
       "      <td>46.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>45.8</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all-mpnet-base-v2</th>\n",
       "      <th>True</th>\n",
       "      <td>11.95</td>\n",
       "      <td>13.79</td>\n",
       "      <td>7.77</td>\n",
       "      <td>9.03</td>\n",
       "      <td>11.19</td>\n",
       "      <td>11.77</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.58</td>\n",
       "      <td>36.83</td>\n",
       "      <td>37.26</td>\n",
       "      <td>...</td>\n",
       "      <td>11.50</td>\n",
       "      <td>13.29</td>\n",
       "      <td>15.30</td>\n",
       "      <td>15.36</td>\n",
       "      <td>7.18</td>\n",
       "      <td>7.31</td>\n",
       "      <td>39.6</td>\n",
       "      <td>15.4</td>\n",
       "      <td>44.4</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <th>False</th>\n",
       "      <td>12.53</td>\n",
       "      <td>13.59</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.14</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.79</td>\n",
       "      <td>27.51</td>\n",
       "      <td>26.24</td>\n",
       "      <td>...</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.98</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.04</td>\n",
       "      <td>25.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-qa-mpnet-base-dot-v1</th>\n",
       "      <th>True</th>\n",
       "      <td>12.97</td>\n",
       "      <td>16.40</td>\n",
       "      <td>7.26</td>\n",
       "      <td>8.68</td>\n",
       "      <td>9.36</td>\n",
       "      <td>10.95</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.53</td>\n",
       "      <td>41.11</td>\n",
       "      <td>43.56</td>\n",
       "      <td>...</td>\n",
       "      <td>10.58</td>\n",
       "      <td>12.90</td>\n",
       "      <td>12.75</td>\n",
       "      <td>14.19</td>\n",
       "      <td>5.09</td>\n",
       "      <td>5.95</td>\n",
       "      <td>44.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>39.2</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     P_P_5  P_P_O  R_P_5  R_P_O  M_P_5  M_P_O  \\\n",
       "model                      embedded                                             \n",
       "CatSeqTG_2RF1              False     12.20  13.78   1.02   1.11   0.89   0.84   \n",
       "ChatGPT                    False      7.73   8.30   0.00   0.00   0.05   0.08   \n",
       "KG-KE-KR-M                 False     15.85  16.91   0.89   1.17   1.14   1.19   \n",
       "KPMiner                    False      7.24   7.96   0.00   0.00   0.00   0.00   \n",
       "KeyBART                    False      8.13   9.60   1.55   1.45   0.94   0.99   \n",
       "MultipartiteRank           False      6.60   7.30   0.13   0.08   0.05   0.08   \n",
       "One2Set                    False     14.00  15.81   1.53   1.65   1.31   1.26   \n",
       "PositionRank               False      6.45   7.12   0.00   0.08   0.00   0.00   \n",
       "TfIdf                      False      7.92   8.06   0.00   0.00   0.00   0.00   \n",
       "TopicRank                  False      6.21   6.40   0.00   0.00   0.00   0.00   \n",
       "YAKE                       False      6.71   7.54   0.00   0.00   0.10   0.17   \n",
       "all-MiniLM-L12-v2          True      13.99  16.45   8.03   8.70  10.64  12.79   \n",
       "all-mpnet-base-v2          True      11.95  13.79   7.77   9.03  11.19  11.77   \n",
       "bart                       False     12.53  13.59   2.13   2.14   1.63   1.83   \n",
       "multi-qa-mpnet-base-dot-v1 True      12.97  16.40   7.26   8.68   9.36  10.95   \n",
       "\n",
       "                                     U_P_5  U_P_O  P_R_5  P_R_O  ...  R_F_5  \\\n",
       "model                      embedded                              ...          \n",
       "CatSeqTG_2RF1              False      0.71   0.66  38.88  36.72  ...   1.44   \n",
       "ChatGPT                    False      0.06   0.04  24.12  21.54  ...   0.00   \n",
       "KG-KE-KR-M                 False      0.53   0.16  49.37  47.80  ...   1.30   \n",
       "KPMiner                    False      0.06   0.04  17.78  17.91  ...   0.00   \n",
       "KeyBART                    False      0.38   0.40  23.23  22.43  ...   2.31   \n",
       "MultipartiteRank           False      0.06   0.04  20.53  19.09  ...   0.16   \n",
       "One2Set                    False      0.53   0.53  43.05  42.06  ...   2.23   \n",
       "PositionRank               False      0.00   0.00  20.26  19.48  ...   0.00   \n",
       "TfIdf                      False      0.12   0.18  25.22  22.80  ...   0.00   \n",
       "TopicRank                  False      0.06   0.04  19.04  17.66  ...   0.00   \n",
       "YAKE                       False      0.00   0.00  23.19  21.90  ...   0.00   \n",
       "all-MiniLM-L12-v2          True       4.91   5.67  43.92  44.48  ...  11.80   \n",
       "all-mpnet-base-v2          True       5.21   5.58  36.83  37.26  ...  11.50   \n",
       "bart                       False      1.35   1.79  27.51  26.24  ...   3.03   \n",
       "multi-qa-mpnet-base-dot-v1 True       3.61   4.53  41.11  43.56  ...  10.58   \n",
       "\n",
       "                                     R_F_O  M_F_5  M_F_O  U_F_5  U_F_O  \\\n",
       "model                      embedded                                      \n",
       "CatSeqTG_2RF1              False      1.70   1.19   1.12   0.96   0.86   \n",
       "ChatGPT                    False      0.00   0.07   0.10   0.10   0.07   \n",
       "KG-KE-KR-M                 False      1.71   1.59   1.62   0.82   0.25   \n",
       "KPMiner                    False      0.00   0.00   0.00   0.10   0.06   \n",
       "KeyBART                    False      2.06   1.26   1.27   0.53   0.51   \n",
       "MultipartiteRank           False      0.12   0.07   0.10   0.10   0.07   \n",
       "One2Set                    False      2.49   1.79   1.66   0.77   0.72   \n",
       "PositionRank               False      0.12   0.00   0.00   0.00   0.00   \n",
       "TfIdf                      False      0.00   0.00   0.00   0.20   0.26   \n",
       "TopicRank                  False      0.00   0.00   0.00   0.10   0.07   \n",
       "YAKE                       False      0.00   0.14   0.20   0.00   0.00   \n",
       "all-MiniLM-L12-v2          True      12.95  14.45  16.61   6.70   7.30   \n",
       "all-mpnet-base-v2          True      13.29  15.30  15.36   7.18   7.31   \n",
       "bart                       False      2.98   2.04   2.18   1.85   2.04   \n",
       "multi-qa-mpnet-base-dot-v1 True      12.90  12.75  14.19   5.09   5.95   \n",
       "\n",
       "                                     P_correct_O  R_correct_O  M_correct_O  \\\n",
       "model                      embedded                                          \n",
       "CatSeqTG_2RF1              False            36.8          2.2          3.8   \n",
       "ChatGPT                    False            23.6          0.0          0.2   \n",
       "KG-KE-KR-M                 False            50.0          2.2          4.4   \n",
       "KPMiner                    False            19.6          0.0          0.0   \n",
       "KeyBART                    False            22.2          1.8          3.4   \n",
       "MultipartiteRank           False            20.8          0.2          0.2   \n",
       "One2Set                    False            42.6          3.0          4.6   \n",
       "PositionRank               False            21.2          0.2          0.0   \n",
       "TfIdf                      False            24.6          0.0          0.0   \n",
       "TopicRank                  False            19.4          0.0          0.0   \n",
       "YAKE                       False            21.6          0.0          0.4   \n",
       "all-MiniLM-L12-v2          True             46.6         15.6         45.8   \n",
       "all-mpnet-base-v2          True             39.6         15.4         44.4   \n",
       "bart                       False            25.2          2.2          4.4   \n",
       "multi-qa-mpnet-base-dot-v1 True             44.0         15.4         39.2   \n",
       "\n",
       "                                     U_correct_O  \n",
       "model                      embedded               \n",
       "CatSeqTG_2RF1              False             2.0  \n",
       "ChatGPT                    False             0.2  \n",
       "KG-KE-KR-M                 False             0.8  \n",
       "KPMiner                    False             0.2  \n",
       "KeyBART                    False             1.0  \n",
       "MultipartiteRank           False             0.2  \n",
       "One2Set                    False             1.4  \n",
       "PositionRank               False             0.0  \n",
       "TfIdf                      False             0.4  \n",
       "TopicRank                  False             0.2  \n",
       "YAKE                       False             0.0  \n",
       "all-MiniLM-L12-v2          True             17.2  \n",
       "all-mpnet-base-v2          True             17.6  \n",
       "bart                       False             2.6  \n",
       "multi-qa-mpnet-base-dot-v1 True             13.2  \n",
       "\n",
       "[15 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "import os\n",
    "results = pandas.concat([pandas.read_csv(f\"benchmark/{file}\", sep=\";\") for file in os.listdir(\"benchmark/\") if \".csv\" in file])\n",
    "(100 * results.drop([\"id\", \"keyphrases\"], axis=1).groupby([\"model\", \"embedded\"]).mean()).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>P_correct_O</th>\n",
       "      <th>R_correct_O</th>\n",
       "      <th>M_correct_O</th>\n",
       "      <th>U_correct_O</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>embedded</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CatSeqTG_2RF1</th>\n",
       "      <th>False</th>\n",
       "      <td>82.25</td>\n",
       "      <td>4.62</td>\n",
       "      <td>8.88</td>\n",
       "      <td>4.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGPT</th>\n",
       "      <th>False</th>\n",
       "      <td>98.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KG-KE-KR-M</th>\n",
       "      <th>False</th>\n",
       "      <td>88.49</td>\n",
       "      <td>3.06</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KPMiner</th>\n",
       "      <th>False</th>\n",
       "      <td>99.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KeyBART</th>\n",
       "      <th>False</th>\n",
       "      <td>77.27</td>\n",
       "      <td>6.61</td>\n",
       "      <td>12.40</td>\n",
       "      <td>3.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultipartiteRank</th>\n",
       "      <th>False</th>\n",
       "      <td>98.03</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One2Set</th>\n",
       "      <th>False</th>\n",
       "      <td>82.77</td>\n",
       "      <td>4.94</td>\n",
       "      <td>9.63</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PositionRank</th>\n",
       "      <th>False</th>\n",
       "      <td>99.47</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TfIdf</th>\n",
       "      <th>False</th>\n",
       "      <td>98.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TopicRank</th>\n",
       "      <th>False</th>\n",
       "      <td>99.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YAKE</th>\n",
       "      <th>False</th>\n",
       "      <td>99.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all-MiniLM-L12-v2</th>\n",
       "      <th>True</th>\n",
       "      <td>38.19</td>\n",
       "      <td>9.87</td>\n",
       "      <td>37.92</td>\n",
       "      <td>14.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all-mpnet-base-v2</th>\n",
       "      <th>True</th>\n",
       "      <td>33.93</td>\n",
       "      <td>11.64</td>\n",
       "      <td>38.58</td>\n",
       "      <td>15.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <th>False</th>\n",
       "      <td>73.20</td>\n",
       "      <td>6.54</td>\n",
       "      <td>12.42</td>\n",
       "      <td>7.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-qa-mpnet-base-dot-v1</th>\n",
       "      <th>True</th>\n",
       "      <td>41.27</td>\n",
       "      <td>11.34</td>\n",
       "      <td>34.30</td>\n",
       "      <td>13.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     P_correct_O  R_correct_O  M_correct_O  \\\n",
       "model                      embedded                                          \n",
       "CatSeqTG_2RF1              False           82.25         4.62         8.88   \n",
       "ChatGPT                    False           98.68         0.00         0.99   \n",
       "KG-KE-KR-M                 False           88.49         3.06         7.60   \n",
       "KPMiner                    False           99.44         0.00         0.00   \n",
       "KeyBART                    False           77.27         6.61        12.40   \n",
       "MultipartiteRank           False           98.03         0.54         1.08   \n",
       "One2Set                    False           82.77         4.94         9.63   \n",
       "PositionRank               False           99.47         0.53         0.00   \n",
       "TfIdf                      False           98.65         0.00         0.00   \n",
       "TopicRank                  False           99.61         0.00         0.00   \n",
       "YAKE                       False           99.04         0.00         0.96   \n",
       "all-MiniLM-L12-v2          True            38.19         9.87        37.92   \n",
       "all-mpnet-base-v2          True            33.93        11.64        38.58   \n",
       "bart                       False           73.20         6.54        12.42   \n",
       "multi-qa-mpnet-base-dot-v1 True            41.27        11.34        34.30   \n",
       "\n",
       "                                     U_correct_O  \n",
       "model                      embedded               \n",
       "CatSeqTG_2RF1              False            4.26  \n",
       "ChatGPT                    False            0.33  \n",
       "KG-KE-KR-M                 False            0.85  \n",
       "KPMiner                    False            0.56  \n",
       "KeyBART                    False            3.72  \n",
       "MultipartiteRank           False            0.36  \n",
       "One2Set                    False            2.67  \n",
       "PositionRank               False            0.00  \n",
       "TfIdf                      False            1.35  \n",
       "TopicRank                  False            0.39  \n",
       "YAKE                       False            0.00  \n",
       "all-MiniLM-L12-v2          True            14.02  \n",
       "all-mpnet-base-v2          True            15.85  \n",
       "bart                       False            7.84  \n",
       "multi-qa-mpnet-base-dot-v1 True            13.09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas.concat([results.loc[results[[f\"{key}_correct_O\" for key in prmu]].sum(axis=1) != 0][[\"model\", \"embedded\"]], \n",
    "               100 * results.loc[results[[f\"{key}_correct_O\" for key in prmu]].sum(axis=1) != 0][[f\"{key}_correct_O\" for key in prmu]].apply(lambda x: x/x.sum(), axis=1)], axis=1).groupby([\"model\", \"embedded\"]).mean().round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
